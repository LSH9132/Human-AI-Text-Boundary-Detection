# KLUE-BERT ê¸°ìˆ  ë¬¸ì„œ

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” ë©”ì¸ í”„ë¡œì íŠ¸ì—ì„œ AUC 0.5 â†’ 0.7355 ëŒíŒŒë¥¼ ë‹¬ì„±í•œ ë°©ë²•ë¡ ì„ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

### í•µì‹¬ êµ¬ì„±ìš”ì†Œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ë°ì´í„° ì²˜ë¦¬   â”‚â”€â”€â”€â–¶â”‚    ëª¨ë¸ í›ˆë ¨     â”‚â”€â”€â”€â–¶â”‚   ì˜ˆì¸¡ ìƒì„±     â”‚
â”‚ KLUEDataProcessorâ”‚    â”‚   KLUETrainer    â”‚    â”‚ KLUEPredictor   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë‹¨ë½ ë¶„í•        â”‚    â”‚ Focal Loss       â”‚    â”‚ ì•™ìƒë¸” ê²°í•©     â”‚
â”‚ í•œêµ­ì–´ ì „ì²˜ë¦¬   â”‚    â”‚ Document-Aware CVâ”‚    â”‚ ë°°ì¹˜ ì²˜ë¦¬       â”‚
â”‚ í† í°í™”          â”‚    â”‚ í˜¼í•© ì •ë°€ë„      â”‚    â”‚ ë©”ëª¨ë¦¬ ìµœì í™”   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ í•µì‹¬ ê¸°ìˆ 

### 1. Focal Loss

í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì˜ í•µì‹¬:

```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.083, gamma=2.0):
        self.alpha = alpha    # AI í´ë˜ìŠ¤ ë¹„ìœ¨ (8.3%)
        self.gamma = gamma    # ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ë„

    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        alpha_t = targets * self.alpha + (1 - targets) * (1 - self.alpha)
        focal_loss = alpha_t * (1 - pt) ** self.gamma * bce_loss
        return focal_loss.mean()
```

**ìˆ˜í•™ì  ë°°ê²½:**
- Î± = 0.083: AI í´ë˜ìŠ¤ ë¹„ìœ¨ ê¸°ë°˜ ê°€ì¤‘ì¹˜
- Î³ = 2.0: ì‰¬ìš´ ìƒ˜í”Œì€ ì–µì œ, ì–´ë ¤ìš´ ìƒ˜í”Œì€ ê°•ì¡°
- PT = e^(-BCE): ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ í™•ë¥ 

### 2. Document-Aware Cross-Validation

ë°ì´í„° ìœ ì¶œ ë°©ì§€ë¥¼ ìœ„í•œ êµì°¨ê²€ì¦:

```python
def create_document_aware_splits(self, processed_df):
    # ë¬¸ì„œë³„ ëŒ€í‘œ ë ˆì´ë¸” ê³„ì‚°
    doc_labels = processed_df.groupby('title')['generated'].agg(lambda x: x.mode().iloc[0])
    
    # GroupKFoldë¡œ ë¬¸ì„œ ë‹¨ìœ„ ë¶„í• 
    group_kfold = GroupKFold(n_splits=self.config.cv.n_folds)
    
    for train_docs, val_docs in group_kfold.split(doc_labels.index, doc_labels.values, doc_labels.index):
        # ë¬¸ì„œ â†’ ë‹¨ë½ ë§¤í•‘
        train_titles = doc_labels.index[train_docs]
        val_titles = doc_labels.index[val_docs]
        
        train_indices = processed_df[processed_df['title'].isin(train_titles)].index
        val_indices = processed_df[processed_df['title'].isin(val_titles)].index
        
        yield train_indices, val_indices
```

**í•µì‹¬ ì›ì¹™:**
- ë™ì¼ ë¬¸ì„œì˜ ë‹¨ë½ë“¤ì€ ê°™ì€ í´ë“œì— ë°°ì¹˜
- ë¬¸ì„œ ë‹¨ìœ„ë¡œ train/validation ë¶„í• 
- ë°ì´í„° ìœ ì¶œ ì™„ì „ ì°¨ë‹¨

### 3. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

KLUE-BERTì— ìµœì í™”ëœ ì „ì²˜ë¦¬:

```python
def clean_korean_text(self, text: str) -> str:
    # í•œêµ­ì–´ ì¹œí™”ì  íŠ¹ìˆ˜ë¬¸ì ì •ì œ
    text = re.sub(r'[^\w\sê°€-í£.,!?;:\'"()[\]{}/-]', ' ', text)
    
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # í•œêµ­ì–´ êµ¬ë‘ì  ì •ë¦¬
    text = re.sub(r'[.]{3,}', '...', text)
    
    return text.strip()

def split_into_paragraphs(self, text: str, title: str = "") -> List[str]:
    # ì¤„ë°”ê¿ˆ ê¸°ì¤€ 1ì°¨ ë¶„í• 
    lines = text.split('\n')
    
    # ì§§ì€ ë¬¸ì¥ë“¤ ë³‘í•© (í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤)
    paragraphs = []
    current_paragraph = ""
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if len(current_paragraph) < 50:  # í•œêµ­ì–´ ìµœì†Œ ê¸¸ì´
            current_paragraph += " " + line if current_paragraph else line
        else:
            paragraphs.append(current_paragraph)
            current_paragraph = line
    
    # í† í° ê¸¸ì´ ê¸°ë°˜ ì¬ë¶„í•  (512 í† í° ì œí•œ)
    filtered_paragraphs = []
    for p in paragraphs:
        if len(p) > self.config.model.max_length * 4:  # í•œêµ­ì–´ í‰ê·  4ê¸€ì/í† í°
            # ë¬¸ì¥ ë‹¨ìœ„ ì¬ë¶„í• 
            sentences = re.split(r'[.!?]\s+', p)
            # ... (ì²­í‚¹ ë¡œì§)
    
    return filtered_paragraphs[:self.config.data.max_paragraphs_per_doc]
```

### 4. ì•™ìƒë¸” ì˜ˆì¸¡

ë‹¤ì¤‘ í´ë“œ ëª¨ë¸ ê²°í•©:

```python
def predict_batch(self, dataset: KLUETextDataset) -> np.ndarray:
    all_predictions = []
    
    with torch.no_grad():
        for batch in dataloader:
            batch_predictions = []
            
            # ê° í´ë“œ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡
            for model_idx, model in enumerate(self.models):
                outputs = model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    token_type_ids=batch.get('token_type_ids')
                )
                
                probs = torch.sigmoid(outputs.logits.squeeze(-1))
                weighted_probs = probs * self.ensemble_weights[model_idx]
                batch_predictions.append(weighted_probs.cpu().numpy())
            
            # ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”
            ensemble_pred = np.sum(batch_predictions, axis=0)
            all_predictions.extend(ensemble_pred)
    
    return np.array(all_predictions)
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

```python
# í˜¼í•© ì •ë°€ë„ í›ˆë ¨
scaler = GradScaler()
with autocast():
    outputs = model(input_ids, attention_mask)
    loss = focal_loss(outputs.logits, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()

# íš¨ìœ¨ì  ë°ì´í„°ë¡œë”©
DataLoader(
    dataset,
    batch_size=16,
    num_workers=4,
    pin_memory=True,  # GPU ì „ì†¡ ìµœì í™”
    persistent_workers=True  # ì›Œì»¤ ì¬ì‚¬ìš©
)
```

### 2. í›ˆë ¨ ì•ˆì •í™”

```python
# ê¸°ìš¸ê¸° í´ë¦¬í•‘
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,
    num_training_steps=total_steps
)

# ì¡°ê¸° ì¢…ë£Œ
early_stopping = EarlyStopping(
    patience=3,
    min_delta=0.001,
    monitor='val_auc'
)
```

## ğŸ”§ ì„¤ì • ì‹œìŠ¤í…œ

### ê³„ì¸µì  ì„¤ì • êµ¬ì¡°

```python
@dataclass
class Config:
    experiment_name: str = "klue_bert_detection"
    seed: int = 42
    
    data: DataConfig = field(default_factory=DataConfig)
    model: ModelConfig = field(default_factory=ModelConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    focal_loss: FocalLossConfig = field(default_factory=FocalLossConfig)
    # ...
```

### YAML ì„¤ì • ì˜ˆì‹œ

```yaml
model:
  name: "klue/bert-base"
  max_length: 512
  
training:
  batch_size: 16
  learning_rate: 2e-5
  epochs: 3
  
focal_loss:
  alpha: 0.083  # AI í´ë˜ìŠ¤ ë¹„ìœ¨
  gamma: 2.0    # ë‚œì´ë„ ì§‘ì¤‘ë„
  
cross_validation:
  n_folds: 3
  strategy: "document_aware"
```

## ğŸ§ª ì¬í˜„ì„± ë³´ì¥

### ì‹œë“œ ê³ ì •

```python
def setup_reproducibility(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

### ë²„ì „ ê³ ì •

```requirements
torch==1.13.0
transformers==4.21.0
scikit-learn==1.1.0
# ... (ëª¨ë“  íŒ¨í‚¤ì§€ ë²„ì „ ê³ ì •)
```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ë©”íŠ¸ë¦­ | ëª©í‘œê°’ | ë‹¬ì„±ê°’ | ì„¤ëª… |
|--------|--------|--------|------|
| CV AUC | 0.735+ | 0.738Â± | 3-fold êµì°¨ê²€ì¦ í‰ê·  |
| ì²˜ë¦¬ì†ë„ | 50+ samples/sec | 90+ samples/sec | ì˜ˆì¸¡ ì²˜ë¦¬ ì†ë„ |
| GPU ë©”ëª¨ë¦¬ | <8GB | ~6GB | ë‹¨ì¼ GPU ìš”êµ¬ì‚¬í•­ |
| í›ˆë ¨ì‹œê°„ | <4ì‹œê°„ | ~3ì‹œê°„ | ì „ì²´ CV í›ˆë ¨ |

## ğŸ” ë””ë²„ê¹… ë° ë¡œê¹…

### ìƒì„¸ ë¡œê¹…

```python
logger.info(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(train_df):,} ë¬¸ì„œ â†’ {len(processed_df):,} ë‹¨ë½")
logger.info(f"ğŸ“ˆ í´ë˜ìŠ¤ ë¶„í¬: AI {ai_ratio:.1%}, Human {human_ratio:.1%}")
logger.info(f"ğŸ¯ Fold {fold} AUC: {auc:.4f}")
```

### ë©”íŠ¸ë¦­ ì¶”ì 

```python
# ì—í¬í¬ë³„ ë©”íŠ¸ë¦­ ì €ì¥
epoch_metrics = {
    'epoch': epoch,
    'train_loss': train_loss,
    'val_auc': val_auc,
    'val_accuracy': val_accuracy,
    'learning_rate': scheduler.get_last_lr()[0]
}
```

ì´ ê¸°ìˆ  ë¬¸ì„œëŠ” ë©”ì¸ í”„ë¡œì íŠ¸ì˜ ì„±ê³µ ë°©ë²•ë¡ ì„ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.