#!/usr/bin/env python3
"""
AI í…ìŠ¤íŠ¸ íŒë³„ ëª¨ë¸ - CUDA ìµœì í™” ë²„ì „
CUDA ì‚¬ìš© ê°€ëŠ¥í•œ í™˜ê²½ì—ì„œ GPU ê°€ì†ì„ í™œìš©í•œ ê³ ì„±ëŠ¥ í•™ìŠµ
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader
import warnings
import gc

warnings.filterwarnings('ignore')


# ========================================
# CUDA ìµœì í™” ì„¤ì • ë³€ìˆ˜ë“¤
# ========================================
MODEL_NAME = 'klue/bert-base'  # í•œêµ­ì–´ BERT ëª¨ë¸
BATCH_SIZE = 16  # GPUì—ì„œ ë” í° ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
EPOCHS = 3  # í•™ìŠµ ì—í¬í¬ ìˆ˜
N_SPLITS = 5  # K-Fold êµì°¨ê²€ì¦ ë¶„í•  ìˆ˜
MAX_LENGTH = 512  # í† í° ìµœëŒ€ ê¸¸ì´
LEARNING_RATE = 2e-5  # í•™ìŠµë¥ 
CONTEXT_WEIGHT = 0.3  # ì»¨í…ìŠ¤íŠ¸ ë³´ì • ê°€ì¤‘ì¹˜
MIXED_PRECISION = True  # Mixed Precision ì‚¬ìš© (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
GRADIENT_ACCUMULATION_STEPS = 2  # ê·¸ë¼ë””ì–¸íŠ¸ ëˆ„ì  (íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ê°€)


def setup_cuda():
    """CUDA í™˜ê²½ ì„¤ì • ë° ìµœì í™”"""
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ë²„ì „(main.py)ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return False, "cpu"
    
    # CUDA ìµœì í™” ì„¤ì •
    torch.backends.cudnn.benchmark = True  # ê³ ì • ì…ë ¥ í¬ê¸°ì— ëŒ€í•œ ìµœì í™”
    torch.backends.cudnn.deterministic = False  # ì„±ëŠ¥ ìš°ì„ 
    
    device = f"cuda:{torch.cuda.current_device()}"
    gpu_name = torch.cuda.get_device_name()
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    
    print(f"ğŸš€ CUDA í™˜ê²½ ì„¤ì • ì™„ë£Œ!")
    print(f"   - GPU: {gpu_name}")
    print(f"   - ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
    print(f"   - ë””ë°”ì´ìŠ¤: {device}")
    print(f"   - Mixed Precision: {MIXED_PRECISION}")
    
    return True, device


def get_optimal_batch_size(device):
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •"""
    if "cuda" not in device:
        return 8
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    
    if gpu_memory >= 24:  # RTX 4090, RTX 6000 Ada ë“±
        return 32
    elif gpu_memory >= 12:  # RTX 4070 Ti, RTX 3080 Ti ë“±
        return 24
    elif gpu_memory >= 8:   # RTX 4060 Ti, RTX 3070 ë“±
        return 16
    else:  # RTX 3060, GTX 1660 ë“±
        return 8


def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
def load_and_preprocess_data():
    """ë°ì´í„° ë¡œë“œ ë° ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• """
    print("ğŸ“Š ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    # í•™ìŠµ ë°ì´í„° ë¡œë“œ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬)
    train_df = pd.read_csv('data/train.csv', encoding='utf-8-sig')
    test_df = pd.read_csv('data/test.csv', encoding='utf-8-sig')

    # í•™ìŠµ ë°ì´í„°ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
    print("âœ‚ï¸  í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    train_paragraphs = []

    for idx, row in train_df.iterrows():
        if idx % 10000 == 0:
            print(f"   ì§„í–‰ë¥ : {idx:,}/{len(train_df):,} ({idx/len(train_df)*100:.1f}%)")
        title = row['title']
        full_text = row['full_text']
        label = row['generated']

        # ë¬¸ë‹¨ ë¶„í•  (ë¹ˆ ì¤„ ê¸°ì¤€)
        paragraphs = [p.strip() for p in full_text.split('\n') if p.strip()]

        for p_idx, paragraph in enumerate(paragraphs):
            train_paragraphs.append({
                'title': title,
                'paragraph_index': p_idx,
                'paragraph_text': paragraph,
                'generated': label,
                'original_idx': idx
            })

    train_para_df = pd.DataFrame(train_paragraphs)

    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
    print(f"   - ì›ë³¸ í›ˆë ¨ ìƒ˜í”Œ: {len(train_df):,}ê°œ")
    print(f"   - í™•ì¥ëœ í›ˆë ¨ ë¬¸ë‹¨: {len(train_para_df):,}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë¬¸ë‹¨: {len(test_df):,}ê°œ")

    return train_para_df, test_df


# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class TextDataset(Dataset):
    def __init__(self, texts, labels=None, tokenizer=None, max_length=MAX_LENGTH):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=MAX_LENGTH,
            return_tensors='pt'
        )

        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }

        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)

        return item


# 3. CUDA ìµœì í™” ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
def train_model_cuda(model, train_loader, val_loader, device, epochs=3, fold=0):
    """CUDA ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ"""
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
    criterion = torch.nn.BCEWithLogitsLoss()
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    total_steps = len(train_loader) * epochs // GRADIENT_ACCUMULATION_STEPS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),  # 10% warmup
        num_training_steps=total_steps
    )

    # Mixed Precision ì„¤ì •
    scaler = torch.cuda.amp.GradScaler() if MIXED_PRECISION else None

    best_auc = 0
    print(f"ğŸ¯ Fold {fold+1} í•™ìŠµ ì‹œì‘ (Mixed Precision: {MIXED_PRECISION})")

    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0
        optimizer.zero_grad()

        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(device, non_blocking=True)
            labels = batch['labels'].to(device, non_blocking=True)

            # Mixed Precision ì‚¬ìš©
            if MIXED_PRECISION:
                with torch.cuda.amp.autocast():
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    loss = criterion(outputs.logits.squeeze(), labels)
                    loss = loss / GRADIENT_ACCUMULATION_STEPS
                
                scaler.scale(loss).backward()
                
                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    scheduler.step()
                    optimizer.zero_grad()
            else:
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                loss = criterion(outputs.logits.squeeze(), labels)
                loss = loss / GRADIENT_ACCUMULATION_STEPS
                loss.backward()
                
                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()

            train_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
            
            # ì§„í–‰ë¥  ì¶œë ¥
            if batch_idx % 100 == 0:
                print(f"   Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()*GRADIENT_ACCUMULATION_STEPS:.4f}")

        # Validation
        model.eval()
        val_preds = []
        val_labels = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device, non_blocking=True)
                attention_mask = batch['attention_mask'].to(device, non_blocking=True)
                labels = batch['labels'].cpu().numpy()

                if MIXED_PRECISION:
                    with torch.cuda.amp.autocast():
                        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                        preds = torch.sigmoid(outputs.logits.squeeze()).cpu().numpy()
                else:
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    preds = torch.sigmoid(outputs.logits.squeeze()).cpu().numpy()

                val_preds.extend(preds)
                val_labels.extend(labels)

        val_auc = roc_auc_score(val_labels, val_preds)

        print(f"ğŸ“ˆ Epoch {epoch + 1}/{epochs}")
        print(f"   Train Loss: {train_loss / len(train_loader):.4f}")
        print(f"   Val AUC: {val_auc:.4f}")
        print(f"   GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB / {torch.cuda.max_memory_allocated()/1e9:.1f}GB")

        if val_auc > best_auc:
            best_auc = val_auc
            torch.save(model.state_dict(), f'best_model_cuda_fold_{fold+1}.pt')
            print(f"   âœ… ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥! AUC: {best_auc:.4f}")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_memory()

    return best_auc


# 4. CUDA ìµœì í™” ì»¨í…ìŠ¤íŠ¸ í™œìš© ì˜ˆì¸¡ í•¨ìˆ˜
def predict_with_context_cuda(model, test_df, tokenizer, device):
    """CUDA ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ í™œìš© ì˜ˆì¸¡"""
    model.eval()
    predictions = []

    # ì œëª©ë³„ë¡œ ê·¸ë£¹í™”
    grouped = test_df.groupby('title')
    total_groups = len(grouped)

    print(f"ğŸ”® ì»¨í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œì‘ ({total_groups}ê°œ ë¬¸ì„œ)")

    with torch.no_grad():
        for group_idx, (title, group) in enumerate(grouped):
            if group_idx % 100 == 0:
                print(f"   ì§„í–‰ë¥ : {group_idx}/{total_groups} ({group_idx/total_groups*100:.1f}%)")
            
            # ê°™ì€ ì œëª©ì˜ ëª¨ë“  ë¬¸ë‹¨ ì˜ˆì¸¡
            paragraphs = group['paragraph_text'].values
            group_preds = []
            
            for para in paragraphs:
                encoding = tokenizer(
                    para,
                    truncation=True,
                    padding='max_length',
                    max_length=MAX_LENGTH,
                    return_tensors='pt'
                )

                input_ids = encoding['input_ids'].to(device, non_blocking=True)
                attention_mask = encoding['attention_mask'].to(device, non_blocking=True)

                if MIXED_PRECISION:
                    with torch.cuda.amp.autocast():
                        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                        pred = torch.sigmoid(outputs.logits.squeeze()).cpu().item()
                else:
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    pred = torch.sigmoid(outputs.logits.squeeze()).cpu().item()
                
                group_preds.append(pred)

            # ì»¨í…ìŠ¤íŠ¸ ë³´ì • (ê°™ì€ ê¸€ì˜ ë¬¸ë‹¨ë“¤ì€ ë¹„ìŠ·í•œ ê²½í–¥)
            avg_pred = np.mean(group_preds)
            adjusted_preds = []

            for pred in group_preds:
                # í‰ê· ê°’ì„ ê³ ë ¤í•œ ë³´ì •
                adjusted = (1 - CONTEXT_WEIGHT) * pred + CONTEXT_WEIGHT * avg_pred
                adjusted_preds.append(adjusted)

            predictions.extend(adjusted_preds)

    return predictions


# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    print("=" * 60)
    print("ğŸ¤– AI í…ìŠ¤íŠ¸ íŒë³„ ëª¨ë¸ - CUDA ê°€ì† ë²„ì „")
    print("=" * 60)
    
    # CUDA í™˜ê²½ ì„¤ì •
    cuda_available, device = setup_cuda()
    if not cuda_available:
        return
    
    # GPUì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ìµœì í™”
    optimal_batch_size = get_optimal_batch_size(device)
    print(f"ğŸ“¦ ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
    
    # ì„¤ì • ì—…ë°ì´íŠ¸
    global BATCH_SIZE
    BATCH_SIZE = optimal_batch_size

    # ë°ì´í„° ë¡œë“œ
    train_para_df, test_df = load_and_preprocess_data()

    # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ”§ í† í¬ë‚˜ì´ì € ë¡œë”©...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # K-Fold Cross Validation
    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)

    oof_predictions = np.zeros(len(train_para_df))
    test_predictions = np.zeros(len(test_df))

    for fold, (train_idx, val_idx) in enumerate(skf.split(train_para_df, train_para_df['generated'])):
        print(f"\nğŸ”„ Fold {fold + 1}/{N_SPLITS} ì‹œì‘")

        # ë°ì´í„° ë¶„í• 
        train_texts = train_para_df.iloc[train_idx]['paragraph_text'].values
        train_labels = train_para_df.iloc[train_idx]['generated'].values
        val_texts = train_para_df.iloc[val_idx]['paragraph_text'].values
        val_labels = train_para_df.iloc[val_idx]['generated'].values

        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = TextDataset(train_texts, train_labels, tokenizer)
        val_dataset = TextDataset(val_texts, val_labels, tokenizer)

        # DataLoaderì— CUDA ìµœì í™” ì˜µì…˜ ì¶”ê°€
        train_loader = DataLoader(
            train_dataset, 
            batch_size=BATCH_SIZE, 
            shuffle=True,
            num_workers=2,  # GPUì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©
            pin_memory=True  # ë©”ëª¨ë¦¬ ê³ ì •ìœ¼ë¡œ ì „ì†¡ ì†ë„ í–¥ìƒ
        )
        val_loader = DataLoader(
            val_dataset, 
            batch_size=BATCH_SIZE, 
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )

        # ëª¨ë¸ ì´ˆê¸°í™”
        print("ğŸ§  ëª¨ë¸ ì´ˆê¸°í™”...")
        model = AutoModelForSequenceClassification.from_pretrained(
            MODEL_NAME,
            num_labels=1
        ).to(device)

        # í•™ìŠµ
        best_auc = train_model_cuda(model, train_loader, val_loader, device, EPOCHS, fold)

        # ìµœê³  ëª¨ë¸ ë¡œë“œ
        model.load_state_dict(torch.load(f'best_model_cuda_fold_{fold+1}.pt'))

        # OOF ì˜ˆì¸¡
        model.eval()
        with torch.no_grad():
            val_preds = []
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device, non_blocking=True)
                attention_mask = batch['attention_mask'].to(device, non_blocking=True)

                if MIXED_PRECISION:
                    with torch.cuda.amp.autocast():
                        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                        preds = torch.sigmoid(outputs.logits.squeeze()).cpu().numpy()
                else:
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                    preds = torch.sigmoid(outputs.logits.squeeze()).cpu().numpy()
                
                val_preds.extend(preds)

            oof_predictions[val_idx] = val_preds

        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (ì»¨í…ìŠ¤íŠ¸ í™œìš©)
        fold_test_preds = predict_with_context_cuda(model, test_df, tokenizer, device)
        test_predictions += np.array(fold_test_preds) / N_SPLITS
        
        print(f"âœ… Fold {fold+1} ì™„ë£Œ! ìµœê³  AUC: {best_auc:.4f}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_gpu_memory()

    # OOF ì ìˆ˜ ê³„ì‚°
    oof_score = roc_auc_score(train_para_df['generated'], oof_predictions)
    print(f"\nğŸ† ìµœì¢… ê²°ê³¼")
    print(f"   OOF AUC Score: {oof_score:.4f}")

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission = pd.read_csv('data/sample_submission.csv', encoding='utf-8-sig')
    submission['generated'] = test_predictions
    submission.to_csv('submission_cuda.csv', index=False)
    print(f"ğŸ“„ ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: submission_cuda.csv")
    
    # ìµœì¢… GPU ë©”ëª¨ë¦¬ ìƒíƒœ
    if torch.cuda.is_available():
        print(f"ğŸ’¾ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.max_memory_allocated()/1e9:.1f}GB")


if __name__ == "__main__":
    main()