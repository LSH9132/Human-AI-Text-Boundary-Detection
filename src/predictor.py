"""
KLUE-BERT ì „ìš© ì˜ˆì¸¡ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ë©”ì¸ í”„ë¡œì íŠ¸ì—ì„œ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„ëœ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ 
ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ì•™ìƒë¸” ì˜ˆì¸¡ (ë‹¤ì¤‘ í´ë“œ ëª¨ë¸ ê²°í•©)
- ë°°ì¹˜ ë‹¨ìœ„ íš¨ìœ¨ì  ì²˜ë¦¬
- ë©”ëª¨ë¦¬ ìµœì í™”
- ê²°ê³¼ í›„ì²˜ë¦¬ ë° ê²€ì¦
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional, Any, Union
import logging
from tqdm import tqdm
import time
import json

from .config import Config
from .data_processor import KLUEDataProcessor, KLUETextDataset
from transformers import AutoModelForSequenceClassification


class KLUEPredictor:
    """KLUE-BERT ì „ìš© ì˜ˆì¸¡ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config):
        """
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device(config.training.device)
        self.logger.info(f"ğŸ”§ ì˜ˆì¸¡ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ê´€ë ¨
        self.models = []
        self.model_paths = []
        self.ensemble_weights = None
        
        # ì„±ëŠ¥ ìµœì í™”
        self.use_mixed_precision = config.optimization.use_mixed_precision
        
        # í†µê³„
        self.prediction_stats = {}
    
    def load_models(self, model_dir: Optional[str] = None) -> int:
        """í›ˆë ¨ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        if model_dir is None:
            model_dir = self.config.output.model_dir
        
        self.logger.info(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_dir}")
        
        # ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
        model_files = []
        if os.path.exists(model_dir):
            for file in os.listdir(model_dir):
                if file.startswith("best_model_fold_") and file.endswith(".pt"):
                    model_files.append(os.path.join(model_dir, file))
        
        model_files.sort()  # í´ë“œ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        
        if not model_files:
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_dir}")
        
        self.logger.info(f"ğŸ” ë°œê²¬ëœ ëª¨ë¸: {len(model_files)}ê°œ")
        
        # ê° ëª¨ë¸ ë¡œë“œ
        for model_path in model_files:
            try:
                self.logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {os.path.basename(model_path)}")
                
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                checkpoint = torch.load(model_path, map_location=self.device)
                
                # ëª¨ë¸ ìƒì„±
                model = AutoModelForSequenceClassification.from_pretrained(
                    self.config.model.name,
                    num_labels=self.config.model.num_labels,
                    problem_type="single_label_classification"
                )
                
                # ê°€ì¤‘ì¹˜ ë¡œë“œ
                model.load_state_dict(checkpoint['model_state_dict'])
                model.to(self.device)
                model.eval()
                
                self.models.append(model)
                self.model_paths.append(model_path)
                
                # ëª¨ë¸ ì„±ëŠ¥ ì •ë³´ ë¡œê¹…
                if 'metrics' in checkpoint:
                    metrics = checkpoint['metrics']
                    self.logger.info(f"   ğŸ“Š ê²€ì¦ AUC: {metrics.get('val_auc', 'N/A'):.4f}")
                
            except Exception as e:
                self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_path}: {e}")
                continue
        
        if not self.models:
            raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        self.logger.info(f"âœ… ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return len(self.models)
    
    def set_ensemble_weights(self, weights: Optional[List[float]] = None):
        """ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì •"""
        if weights is None:
            # ê· ë“± ê°€ì¤‘ì¹˜ ì‚¬ìš©
            weights = [1.0 / len(self.models)] * len(self.models)
        
        if len(weights) != len(self.models):
            raise ValueError(f"ê°€ì¤‘ì¹˜ ê°œìˆ˜({len(weights)})ì™€ ëª¨ë¸ ê°œìˆ˜({len(self.models)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ì •ê·œí™”
        total_weight = sum(weights)
        self.ensemble_weights = [w / total_weight for w in weights]
        
        self.logger.info(f"âš–ï¸ ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {[f'{w:.3f}' for w in self.ensemble_weights]}")
    
    def predict_batch(self, dataset: KLUETextDataset) -> np.ndarray:
        """ë°°ì¹˜ ë‹¨ìœ„ ì˜ˆì¸¡"""
        if not self.models:
            raise RuntimeError("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. load_models()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì • (ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
        if self.ensemble_weights is None:
            self.set_ensemble_weights()
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.prediction.batch_size,
            shuffle=False,
            num_workers=self.config.optimization.dataloader_num_workers,
            pin_memory=self.config.optimization.pin_memory
        )
        
        self.logger.info(f"ğŸ¯ ì˜ˆì¸¡ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {self.config.prediction.batch_size})")
        
        all_predictions = []
        total_samples = 0
        start_time = time.time()
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(dataloader, desc="ì˜ˆì¸¡ ì§„í–‰")):
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch = {k: v.to(self.device) for k, v in batch.items() if k != 'labels'}
                
                batch_predictions = []
                
                # ê° ëª¨ë¸ì—ì„œ ì˜ˆì¸¡
                for model_idx, model in enumerate(self.models):
                    if self.use_mixed_precision:
                        with autocast():
                            outputs = model(
                                input_ids=batch['input_ids'],
                                attention_mask=batch['attention_mask'],
                                token_type_ids=batch.get('token_type_ids')
                            )
                    else:
                        outputs = model(
                            input_ids=batch['input_ids'],
                            attention_mask=batch['attention_mask'],
                            token_type_ids=batch.get('token_type_ids')
                        )
                    
                    # ì‹œê·¸ëª¨ì´ë“œ ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜
                    logits = outputs.logits.squeeze(-1)
                    probs = torch.sigmoid(logits)
                    
                    # ê°€ì¤‘ì¹˜ ì ìš©
                    weighted_probs = probs * self.ensemble_weights[model_idx]
                    batch_predictions.append(weighted_probs.cpu().numpy())
                
                # ì•™ìƒë¸” ê²°í•©
                if self.config.prediction.ensemble_method == "average":
                    ensemble_pred = np.sum(batch_predictions, axis=0)
                elif self.config.prediction.ensemble_method == "weighted":
                    ensemble_pred = np.sum(batch_predictions, axis=0)
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•™ìƒë¸” ë°©ë²•: {self.config.prediction.ensemble_method}")
                
                all_predictions.extend(ensemble_pred)
                total_samples += len(ensemble_pred)
        
        prediction_time = time.time() - start_time
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.prediction_stats = {
            'total_samples': total_samples,
            'prediction_time': prediction_time,
            'samples_per_second': total_samples / prediction_time,
            'num_models': len(self.models),
            'batch_size': self.config.prediction.batch_size
        }
        
        self.logger.info(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {total_samples:,} ìƒ˜í”Œ ({prediction_time:.1f}ì´ˆ)")
        self.logger.info(f"ğŸ“ˆ ì²˜ë¦¬ ì†ë„: {self.prediction_stats['samples_per_second']:.1f} ìƒ˜í”Œ/ì´ˆ")
        
        return np.array(all_predictions)
    
    def predict_test_data(self, processor: KLUEDataProcessor, 
                         test_df: pd.DataFrame) -> pd.DataFrame:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡"""
        self.logger.info("ğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬
        test_texts = []
        test_ids = []
        
        for idx, row in test_df.iterrows():
            # ë‹¨ë½ ë¶„í• 
            paragraphs = processor.split_into_paragraphs(
                row['full_text'], 
                row['title']
            )
            
            if not paragraphs:
                # ë¹ˆ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ì›ë³¸ ì‚¬ìš©
                paragraphs = [str(row['full_text'])]
            
            # ê° ë‹¨ë½ ì¶”ê°€
            for para in paragraphs:
                test_texts.append(para)
                test_ids.append(row['ID'] if 'ID' in row else f"TEST_{idx:04d}")
        
        self.logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë‹¨ë½ ìˆ˜: {len(test_texts):,}")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        test_dataset = processor.create_dataset(test_texts)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = self.predict_batch(test_dataset)
        
        # ê²°ê³¼ ì •ë¦¬
        results_df = pd.DataFrame({
            'ID': test_ids,
            'paragraph_text': test_texts,
            'prediction': predictions
        })
        
        # ë¬¸ì„œë³„ ì˜ˆì¸¡ ì§‘ê³„ (í‰ê· )
        if len(set(test_ids)) < len(test_ids):  # ì—¬ëŸ¬ ë‹¨ë½ì´ ìˆëŠ” ê²½ìš°
            self.logger.info("ğŸ“Š ë¬¸ì„œë³„ ì˜ˆì¸¡ ì§‘ê³„ ì¤‘...")
            
            final_predictions = []
            unique_ids = test_df['ID'].tolist() if 'ID' in test_df.columns else [f"TEST_{i:04d}" for i in range(len(test_df))]
            
            for test_id in unique_ids:
                doc_predictions = results_df[results_df['ID'] == test_id]['prediction'].values
                if len(doc_predictions) > 0:
                    # ë¬¸ì„œ ë‚´ ë‹¨ë½ë“¤ì˜ í‰ê·  ì˜ˆì¸¡ê°’
                    final_pred = np.mean(doc_predictions)
                else:
                    final_pred = 0.5  # ê¸°ë³¸ê°’
                
                final_predictions.append(final_pred)
            
            # ìµœì¢… ê²°ê³¼ ìƒì„±
            submission_df = pd.DataFrame({
                'ID': unique_ids,
                'generated': final_predictions
            })
        else:
            # ë‹¨ë½ë³„ ì˜ˆì¸¡ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            submission_df = pd.DataFrame({
                'ID': test_ids,
                'generated': predictions
            })
        
        self.logger.info(f"ğŸ“ ìµœì¢… ì˜ˆì¸¡ ìˆ˜: {len(submission_df):,}")
        
        return submission_df
    
    def save_predictions(self, predictions_df: pd.DataFrame, 
                        output_path: Optional[str] = None) -> str:
        """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""
        if output_path is None:
            output_path = self.config.output.submission_file
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # CSV ì €ì¥
        predictions_df.to_csv(output_path, index=False)
        
        self.logger.info(f"ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # í†µê³„ ë¡œê¹…
        pred_stats = self._analyze_predictions(predictions_df['generated'].values)
        self.logger.info(f"ğŸ“Š ì˜ˆì¸¡ í†µê³„: {pred_stats}")
        
        return output_path
    
    def _analyze_predictions(self, predictions: np.ndarray) -> Dict[str, Any]:
        """ì˜ˆì¸¡ ê²°ê³¼ í†µê³„ ë¶„ì„"""
        return {
            'count': len(predictions),
            'mean': float(np.mean(predictions)),
            'std': float(np.std(predictions)),
            'min': float(np.min(predictions)),
            'max': float(np.max(predictions)),
            'q25': float(np.percentile(predictions, 25)),
            'median': float(np.median(predictions)),
            'q75': float(np.percentile(predictions, 75)),
            'ai_ratio_05': float(np.mean(predictions > 0.5)),
            'ai_ratio_03': float(np.mean(predictions > 0.3)),
            'ai_ratio_07': float(np.mean(predictions > 0.7))
        }
    
    def get_prediction_stats(self) -> Dict[str, Any]:
        """ì˜ˆì¸¡ í†µê³„ ë°˜í™˜"""
        return self.prediction_stats.copy()
    
    def validate_predictions(self, predictions_df: pd.DataFrame, 
                           sample_submission_path: Optional[str] = None) -> bool:
        """ì˜ˆì¸¡ ê²°ê³¼ ê²€ì¦"""
        try:
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_cols = ['ID', 'generated']
            missing_cols = [col for col in required_cols if col not in predictions_df.columns]
            
            if missing_cols:
                self.logger.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
                return False
            
            # ì˜ˆì¸¡ê°’ ë²”ìœ„ í™•ì¸
            predictions = predictions_df['generated'].values
            if np.any(predictions < 0) or np.any(predictions > 1):
                self.logger.error("âŒ ì˜ˆì¸¡ê°’ì´ [0, 1] ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
                return False
            
            # NaN í™•ì¸
            if predictions_df.isnull().sum().sum() > 0:
                self.logger.error("âŒ NaN ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return False
            
            # ìƒ˜í”Œ ì œì¶œ íŒŒì¼ê³¼ ë¹„êµ
            if sample_submission_path and os.path.exists(sample_submission_path):
                sample_df = pd.read_csv(sample_submission_path)
                
                if len(predictions_df) != len(sample_df):
                    self.logger.error(f"âŒ ì˜ˆì¸¡ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤: {len(predictions_df)} vs {len(sample_df)}")
                    return False
                
                # ID ìˆœì„œ í™•ì¸
                if not predictions_df['ID'].equals(sample_df['ID']):
                    self.logger.warning("âš ï¸ ID ìˆœì„œê°€ ë‹¤ë¦…ë‹ˆë‹¤. ì •ë ¬ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
            self.logger.info("âœ… ì˜ˆì¸¡ ê²°ê³¼ ê²€ì¦ í†µê³¼")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False


def create_submission(config_path: str = "config.yaml", 
                     test_data_path: Optional[str] = None,
                     output_path: Optional[str] = None) -> str:
    """ì œì¶œ íŒŒì¼ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    from .config import load_config
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(config_path)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ ì„¤ì •
    if test_data_path:
        config.data.test_file = test_data_path
    
    # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    if output_path:
        config.output.submission_file = output_path
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ KLUE-BERT ì˜ˆì¸¡ ì‹œì‘")
    
    # ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„±
    processor = KLUEDataProcessor(config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    _, test_df = processor.load_data()
    
    # ì˜ˆì¸¡ê¸° ìƒì„±
    predictor = KLUEPredictor(config)
    
    # ëª¨ë¸ ë¡œë“œ
    predictor.load_models()
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    predictions_df = predictor.predict_test_data(processor, test_df)
    
    # ê²°ê³¼ ì €ì¥
    output_file = predictor.save_predictions(predictions_df)
    
    # ê²€ì¦
    predictor.validate_predictions(predictions_df, config.data.submission_file)
    
    logger.info(f"ğŸ‰ ì˜ˆì¸¡ ì™„ë£Œ: {output_file}")
    return output_file


def test_predictor():
    """ì˜ˆì¸¡ê¸° í…ŒìŠ¤íŠ¸"""
    from .config import Config
    
    print("ğŸ§ª KLUE ì˜ˆì¸¡ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì„¤ì • ë¡œë“œ
    config = Config()
    config.prediction.batch_size = 4
    
    # ì˜ˆì¸¡ê¸° ìƒì„±
    predictor = KLUEPredictor(config)
    
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {predictor.device}")
    print(f"âš–ï¸ ì•™ìƒë¸” ë°©ë²•: {config.prediction.ensemble_method}")
    
    print("âœ… ì˜ˆì¸¡ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    test_predictor()