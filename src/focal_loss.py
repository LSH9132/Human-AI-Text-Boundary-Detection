"""
Focal Loss êµ¬í˜„ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë©”ì¸ í”„ë¡œì íŠ¸ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ëœ Focal Lossë¥¼ 
ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°ì— í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.

ì°¸ê³ : ë©”ì¸ í”„ë¡œì íŠ¸ì—ì„œ AUC 0.5 â†’ 0.7355 ëŒíŒŒì˜ í•µì‹¬ ê¸°ìˆ 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class FocalLoss(nn.Module):
    """
    Focal Loss êµ¬í˜„
    
    í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê³ ê¸‰ ì†ì‹¤ í•¨ìˆ˜
    - alpha: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ì†Œìˆ˜ í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
    - gamma: ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ëŠ” ì •ë„
    
    ë©”ì¸ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©ëœ ì„±ê³µì ì¸ íŒŒë¼ë¯¸í„°:
    - alpha = 0.083 (AI í´ë˜ìŠ¤ ë¹„ìœ¨ 8.2% ê¸°ë°˜)
    - gamma = 2.0 (í‘œì¤€ Focal Loss ê°’)
    """
    
    def __init__(self, alpha: float = 0.083, gamma: float = 2.0, reduction: str = 'mean'):
        """
        Args:
            alpha: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (0-1 ì‚¬ì´ ê°’, ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ë¹„ìœ¨)
            gamma: focusing parameter (ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ëŠ” ì •ë„)
            reduction: ì†ì‹¤ ì¶•ì†Œ ë°©ë²• ('mean', 'sum', 'none')
        """
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
        # ë¡œê·¸ì— ì‚¬ìš©í•  ì •ë³´ ì €ì¥
        self.num_positive = 0
        self.num_negative = 0
        self.total_loss = 0.0
        self.batch_count = 0
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Focal Loss ê³„ì‚°
        
        Args:
            inputs: ëª¨ë¸ ì¶œë ¥ logits [batch_size, 1] ë˜ëŠ” [batch_size]
            targets: ì‹¤ì œ ë ˆì´ë¸” [batch_size] (0 ë˜ëŠ” 1)
        
        Returns:
            focal_loss: ê³„ì‚°ëœ Focal Loss
        """
        # ì…ë ¥ ì°¨ì› ì •ê·œí™”
        if inputs.dim() > 1:
            inputs = inputs.squeeze(-1)
        
        if targets.dim() > 1:
            targets = targets.squeeze(-1)
        
        # BCE loss ê³„ì‚°
        bce_loss = F.binary_cross_entropy_with_logits(
            inputs, targets.float(), reduction='none'
        )
        
        # í™•ë¥  ê³„ì‚°
        pt = torch.exp(-bce_loss)
        
        # Alpha ê°€ì¤‘ì¹˜ ê³„ì‚°
        # ì–‘ì„± í´ë˜ìŠ¤(AI)ì—ëŠ” alpha, ìŒì„± í´ë˜ìŠ¤(Human)ì—ëŠ” (1-alpha)
        alpha_t = targets * self.alpha + (1 - targets) * (1 - self.alpha)
        
        # Focal Loss ê³„ì‚°
        focal_loss = alpha_t * (1 - pt) ** self.gamma * bce_loss
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_stats(targets, focal_loss)
        
        # ì¶•ì†Œ ë°©ë²•ì— ë”°ë¥¸ ìµœì¢… ì†ì‹¤ ê³„ì‚°
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss
    
    def _update_stats(self, targets: torch.Tensor, loss: torch.Tensor):
        """í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸"""
        with torch.no_grad():
            self.num_positive += targets.sum().item()
            self.num_negative += (1 - targets).sum().item()
            self.total_loss += loss.sum().item()
            self.batch_count += 1
    
    def get_stats(self) -> dict:
        """ì†ì‹¤ í†µê³„ ë°˜í™˜"""
        total_samples = self.num_positive + self.num_negative
        
        return {
            'positive_samples': int(self.num_positive),
            'negative_samples': int(self.num_negative),
            'total_samples': int(total_samples),
            'positive_ratio': self.num_positive / total_samples if total_samples > 0 else 0.0,
            'avg_loss': self.total_loss / total_samples if total_samples > 0 else 0.0,
            'batch_count': self.batch_count,
            'alpha': self.alpha,
            'gamma': self.gamma
        }
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.num_positive = 0
        self.num_negative = 0
        self.total_loss = 0.0
        self.batch_count = 0


class WeightedFocalLoss(FocalLoss):
    """
    ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ Focal Loss
    
    í´ë˜ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•  ìˆ˜ ìˆëŠ” í™•ì¥ëœ ë²„ì „
    """
    
    def __init__(self, class_weights: Optional[torch.Tensor] = None, 
                 alpha: float = 0.083, gamma: float = 2.0, reduction: str = 'mean'):
        """
        Args:
            class_weights: í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ [negative_weight, positive_weight]
            alpha: Focal Lossì˜ alpha íŒŒë¼ë¯¸í„°
            gamma: Focal Lossì˜ gamma íŒŒë¼ë¯¸í„°
            reduction: ì†ì‹¤ ì¶•ì†Œ ë°©ë²•
        """
        super().__init__(alpha, gamma, reduction)
        self.class_weights = class_weights
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ Focal Loss ê³„ì‚°"""
        # ê¸°ë³¸ Focal Loss ê³„ì‚°
        focal_loss = super().forward(inputs, targets)
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
        if self.class_weights is not None:
            if self.class_weights.device != targets.device:
                self.class_weights = self.class_weights.to(targets.device)
            
            # ê° ìƒ˜í”Œì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ ì„ íƒ
            weights = self.class_weights[targets.long()]
            
            if self.reduction == 'none':
                focal_loss = focal_loss * weights
            else:
                # meanì´ë‚˜ sumì˜ ê²½ìš° ê°€ì¤‘í‰ê·  ì ìš©
                weighted_loss = focal_loss * weights
                if self.reduction == 'mean':
                    focal_loss = weighted_loss.sum() / weights.sum()
                else:  # sum
                    focal_loss = weighted_loss.sum()
        
        return focal_loss


def create_focal_loss(alpha: float = 0.083, gamma: float = 2.0, 
                     class_weights: Optional[torch.Tensor] = None) -> nn.Module:
    """
    Focal Loss ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Args:
        alpha: í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • íŒŒë¼ë¯¸í„°
        gamma: ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ íŒŒë¼ë¯¸í„°
        class_weights: ì¶”ê°€ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ì„ íƒì‚¬í•­)
    
    Returns:
        focal_loss: ì„¤ì •ëœ Focal Loss ê°ì²´
    """
    if class_weights is not None:
        return WeightedFocalLoss(class_weights, alpha, gamma)
    else:
        return FocalLoss(alpha, gamma)


def calculate_optimal_alpha(positive_ratio: float) -> float:
    """
    ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì  alpha ê°’ ê³„ì‚°
    
    Args:
        positive_ratio: ì–‘ì„± í´ë˜ìŠ¤(AI)ì˜ ë¹„ìœ¨ (0-1)
    
    Returns:
        optimal_alpha: ê³„ì‚°ëœ ìµœì  alpha ê°’
    """
    # ì¼ë°˜ì ìœ¼ë¡œ ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ì„ alphaë¡œ ì‚¬ìš©
    return positive_ratio


def test_focal_loss():
    """Focal Loss ë™ì‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Focal Loss í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size = 100
    inputs = torch.randn(batch_size, 1)
    targets = torch.randint(0, 2, (batch_size,)).float()
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    positive_ratio = targets.mean().item()
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ì–‘ì„± ë¹„ìœ¨: {positive_ratio:.3f}")
    
    # Focal Loss ìƒì„± ë° í…ŒìŠ¤íŠ¸
    focal_loss = FocalLoss(alpha=positive_ratio, gamma=2.0)
    
    # ì†ì‹¤ ê³„ì‚°
    loss = focal_loss(inputs, targets)
    print(f"ğŸ’¯ Focal Loss: {loss.item():.4f}")
    
    # í†µê³„ í™•ì¸
    stats = focal_loss.get_stats()
    print(f"ğŸ“ˆ í†µê³„: {stats}")
    
    # BCEì™€ ë¹„êµ
    bce_loss = F.binary_cross_entropy_with_logits(inputs.squeeze(), targets)
    print(f"ğŸ” BCE Loss (ì°¸ê³ ): {bce_loss.item():.4f}")
    
    print("âœ… Focal Loss í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    test_focal_loss()