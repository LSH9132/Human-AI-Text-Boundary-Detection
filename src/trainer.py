"""
KLUE-BERT ì „ìš© ëª¨ë¸ í›ˆë ¨ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë©”ì¸ í”„ë¡œì íŠ¸ì—ì„œ AUC 0.7355 ëŒíŒŒë¥¼ ë‹¬ì„±í•œ 
í›ˆë ¨ ë°©ë²•ë¡ ì„ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- KLUE-BERT ëª¨ë¸ ì´ˆê¸°í™” ë° ì„¤ì •
- Focal Lossë¥¼ í™œìš©í•œ ë¶ˆê· í˜• ë°ì´í„° í›ˆë ¨
- Document-Aware êµì°¨ê²€ì¦
- í˜¼í•© ì •ë°€ë„ í›ˆë ¨
- ì¡°ê¸° ì¢…ë£Œ ë° ëª¨ë¸ ì €ì¥
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.cuda.amp import GradScaler, autocast
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    get_linear_schedule_with_warmup
)
import numpy as np
from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support
from typing import Dict, List, Tuple, Optional, Any
import logging
from tqdm import tqdm
import time
import json

from .config import Config
from .focal_loss import FocalLoss
from .data_processor import KLUEDataProcessor, KLUETextDataset


class EarlyStopping:
    """ì¡°ê¸° ì¢…ë£Œ í´ë˜ìŠ¤"""
    
    def __init__(self, patience: int = 3, min_delta: float = 0.001, 
                 monitor: str = 'val_auc', mode: str = 'max'):
        """
        Args:
            patience: ì„±ëŠ¥ ê°œì„ ì´ ì—†ì„ ë•Œ ê¸°ë‹¤ë¦´ ì—í¬í¬ ìˆ˜
            min_delta: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰
            monitor: ëª¨ë‹ˆí„°ë§í•  ë©”íŠ¸ë¦­
            mode: 'max' (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) ë˜ëŠ” 'min' (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        self.patience = patience
        self.min_delta = min_delta
        self.monitor = monitor
        self.mode = mode
        
        self.best_score = None
        self.counter = 0
        self.early_stop = False
        
        self.best_epoch = 0
        self.best_metrics = {}
    
    def __call__(self, metrics: Dict[str, float], epoch: int) -> bool:
        """
        í˜„ì¬ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€ ê²°ì •
        
        Returns:
            bool: True if early stopping should be triggered
        """
        score = metrics.get(self.monitor)
        if score is None:
            return False
        
        if self.best_score is None:
            self.best_score = score
            self.best_epoch = epoch
            self.best_metrics = metrics.copy()
            return False
        
        # ê°œì„  ì—¬ë¶€ í™•ì¸
        if self.mode == 'max':
            improved = score > self.best_score + self.min_delta
        else:
            improved = score < self.best_score - self.min_delta
        
        if improved:
            self.best_score = score
            self.counter = 0
            self.best_epoch = epoch
            self.best_metrics = metrics.copy()
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        
        return self.early_stop


class KLUETrainer:
    """KLUE-BERT ì „ìš© í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config):
        """
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device(config.training.device)
        self.logger.info(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = None
        self.tokenizer = None
        self.optimizer = None
        self.scheduler = None
        self.loss_fn = None
        
        # í›ˆë ¨ ìƒíƒœ
        self.scaler = GradScaler() if config.optimization.use_mixed_precision else None
        self.global_step = 0
        self.current_fold = 0
        
        # ê²°ê³¼ ì €ì¥
        self.fold_results = []
        self.training_history = []
        
        self._setup_model()
        self._setup_logging()
    
    def _setup_model(self):
        """ëª¨ë¸ ë° ê´€ë ¨ êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”"""
        self.logger.info(f"ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™”: {self.config.model.name}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model.name,
            use_fast=True
        )
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.config.model.name,
            num_labels=self.config.model.num_labels,
            problem_type="single_label_classification"
        )
        
        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.model.to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        self.loss_fn = FocalLoss(
            alpha=self.config.focal_loss.alpha,
            gamma=self.config.focal_loss.gamma
        )
        
        self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,})")
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        os.makedirs(self.config.output.log_dir, exist_ok=True)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
        log_file = os.path.join(self.config.output.log_dir, "training.log")
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        
        self.logger.addHandler(file_handler)
    
    def _setup_optimizer_scheduler(self, train_dataloader: DataLoader):
        """ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=self.config.training.learning_rate,
            weight_decay=self.config.training.weight_decay
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        total_steps = len(train_dataloader) * self.config.training.epochs
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=self.config.training.warmup_steps,
            num_training_steps=total_steps
        )
        
        self.logger.info(f"ğŸ“Š ì´ í›ˆë ¨ ìŠ¤í…: {total_steps:,}")
        self.logger.info(f"ğŸ”¥ ì›Œë°ì—… ìŠ¤í…: {self.config.training.warmup_steps:,}")
    
    def train_epoch(self, train_dataloader: DataLoader) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        self.model.train()
        
        total_loss = 0.0
        total_samples = 0
        
        pbar = tqdm(train_dataloader, desc=f"Fold {self.current_fold} í›ˆë ¨")
        
        for batch in pbar:
            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
            self.optimizer.zero_grad()
            
            # ìˆœì „íŒŒ
            if self.scaler is not None:
                with autocast():
                    outputs = self.model(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        token_type_ids=batch.get('token_type_ids')
                    )
                    logits = outputs.logits.squeeze(-1)
                    loss = self.loss_fn(logits, batch['labels'])
                
                # ì—­ì „íŒŒ
                self.scaler.scale(loss).backward()
                
                # ê¸°ìš¸ê¸° í´ë¦¬í•‘
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config.training.max_grad_norm
                )
                
                # ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    token_type_ids=batch.get('token_type_ids')
                )
                logits = outputs.logits.squeeze(-1)
                loss = self.loss_fn(logits, batch['labels'])
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê¸°ìš¸ê¸° í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config.training.max_grad_norm
                )
                
                # ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸
                self.optimizer.step()
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += loss.item() * len(batch['labels'])
            total_samples += len(batch['labels'])
            self.global_step += 1
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'avg_loss': f"{total_loss/total_samples:.4f}",
                'lr': f"{self.scheduler.get_last_lr()[0]:.2e}"
            })
        
        avg_loss = total_loss / total_samples
        return {'train_loss': avg_loss}
    
    def evaluate(self, val_dataloader: DataLoader) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        self.model.eval()
        
        all_predictions = []
        all_labels = []
        total_loss = 0.0
        total_samples = 0
        
        with torch.no_grad():
            pbar = tqdm(val_dataloader, desc=f"Fold {self.current_fold} í‰ê°€")
            
            for batch in pbar:
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # ìˆœì „íŒŒ
                if self.scaler is not None:
                    with autocast():
                        outputs = self.model(
                            input_ids=batch['input_ids'],
                            attention_mask=batch['attention_mask'],
                            token_type_ids=batch.get('token_type_ids')
                        )
                else:
                    outputs = self.model(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask'],
                        token_type_ids=batch.get('token_type_ids')
                    )
                
                logits = outputs.logits.squeeze(-1)
                loss = self.loss_fn(logits, batch['labels'])
                
                # ì˜ˆì¸¡ê°’ ê³„ì‚°
                predictions = torch.sigmoid(logits)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(batch['labels'].cpu().numpy())
                
                total_loss += loss.item() * len(batch['labels'])
                total_samples += len(batch['labels'])
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        all_predictions = np.array(all_predictions)
        all_labels = np.array(all_labels)
        
        auc = roc_auc_score(all_labels, all_predictions)
        
        # ì„ê³„ê°’ 0.5ë¡œ ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°
        binary_predictions = (all_predictions > 0.5).astype(int)
        accuracy = accuracy_score(all_labels, binary_predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, binary_predictions, average='binary'
        )
        
        avg_loss = total_loss / total_samples
        
        return {
            'val_loss': avg_loss,
            'val_auc': auc,
            'val_accuracy': accuracy,
            'val_precision': precision,
            'val_recall': recall,
            'val_f1': f1
        }
    
    def train_fold(self, train_dataset: KLUETextDataset, 
                   val_dataset: KLUETextDataset, fold_idx: int) -> Dict[str, Any]:
        """ë‹¨ì¼ í´ë“œ í›ˆë ¨"""
        self.current_fold = fold_idx + 1
        
        self.logger.info(f"\n{'='*20} Fold {self.current_fold}/{self.config.cv.n_folds} {'='*20}")
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=self.config.training.batch_size,
            shuffle=True,
            num_workers=self.config.optimization.dataloader_num_workers,
            pin_memory=self.config.optimization.pin_memory
        )
        
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=self.config.prediction.batch_size,
            shuffle=False,
            num_workers=self.config.optimization.dataloader_num_workers,
            pin_memory=self.config.optimization.pin_memory
        )
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self._setup_optimizer_scheduler(train_dataloader)
        
        # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
        early_stopping = EarlyStopping(
            patience=self.config.early_stopping.patience,
            min_delta=self.config.early_stopping.min_delta,
            monitor=self.config.early_stopping.monitor
        )
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ë§¤ í´ë“œë§ˆë‹¤ ìƒˆë¡œ ì‹œì‘)
        self._setup_model()
        
        # í´ë˜ìŠ¤ ë¶„í¬ ë¡œê¹…
        train_stats = train_dataset.get_stats()
        val_stats = val_dataset.get_stats()
        
        self.logger.info(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {train_stats['total_samples']:,} ìƒ˜í”Œ")
        self.logger.info(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {val_stats['total_samples']:,} ìƒ˜í”Œ")
        self.logger.info(f"ğŸ“ˆ í›ˆë ¨ AI ë¹„ìœ¨: {train_stats['positive_ratio']:.1%}")
        self.logger.info(f"ğŸ“ˆ ê²€ì¦ AI ë¹„ìœ¨: {val_stats['positive_ratio']:.1%}")
        
        # í›ˆë ¨ ì‹œì‘
        best_model_state = None
        fold_history = []
        
        start_time = time.time()
        
        for epoch in range(self.config.training.epochs):
            epoch_start = time.time()
            
            self.logger.info(f"\n--- Epoch {epoch + 1}/{self.config.training.epochs} ---")
            
            # í›ˆë ¨
            train_metrics = self.train_epoch(train_dataloader)
            
            # í‰ê°€
            val_metrics = self.evaluate(val_dataloader)
            
            # ë©”íŠ¸ë¦­ í•©ì¹˜ê¸°
            epoch_metrics = {**train_metrics, **val_metrics}
            epoch_metrics['epoch'] = epoch + 1
            epoch_metrics['fold'] = self.current_fold
            epoch_metrics['epoch_time'] = time.time() - epoch_start
            
            fold_history.append(epoch_metrics)
            
            # ë¡œê¹…
            self.logger.info(
                f"ğŸƒ í›ˆë ¨ ì†ì‹¤: {train_metrics['train_loss']:.4f} | "
                f"ğŸ¯ ê²€ì¦ AUC: {val_metrics['val_auc']:.4f} | "
                f"ğŸ“Š ê²€ì¦ ì†ì‹¤: {val_metrics['val_loss']:.4f}"
            )
            
            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if early_stopping(val_metrics, epoch):
                self.logger.info(f"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ (patience={early_stopping.patience})")
                break
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if val_metrics['val_auc'] == early_stopping.best_score:
                best_model_state = self.model.state_dict().copy()
                
                if self.config.output.save_best_only:
                    model_path = os.path.join(
                        self.config.output.model_dir,
                        f"best_model_fold_{self.current_fold}.pt"
                    )
                    torch.save({
                        'model_state_dict': best_model_state,
                        'config': self.config.to_dict(),
                        'metrics': val_metrics,
                        'epoch': epoch + 1,
                        'fold': self.current_fold
                    }, model_path)
                    
                    self.logger.info(f"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥: {model_path}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
        
        training_time = time.time() - start_time
        
        # í´ë“œ ê²°ê³¼ ì •ë¦¬
        fold_result = {
            'fold': self.current_fold,
            'best_metrics': early_stopping.best_metrics,
            'best_epoch': early_stopping.best_epoch,
            'training_time': training_time,
            'history': fold_history
        }
        
        self.logger.info(f"âœ… Fold {self.current_fold} ì™„ë£Œ (ì‹œê°„: {training_time:.1f}ì´ˆ)")
        self.logger.info(f"ğŸ† ìµœê³  AUC: {early_stopping.best_metrics['val_auc']:.4f}")
        
        return fold_result
    
    def cross_validate(self, processor: KLUEDataProcessor, 
                      processed_df) -> Dict[str, Any]:
        """êµì°¨ê²€ì¦ í›ˆë ¨"""
        self.logger.info("ğŸš€ KLUE-BERT êµì°¨ê²€ì¦ í›ˆë ¨ ì‹œì‘")
        
        # Document-Aware ë¶„í•  ìƒì„±
        splits = processor.create_document_aware_splits(processed_df)
        
        # ê° í´ë“œ í›ˆë ¨
        for fold_idx, (train_indices, val_indices) in enumerate(splits):
            # ë°ì´í„° ë¶„í• 
            train_data = processed_df.iloc[train_indices]
            val_data = processed_df.iloc[val_indices]
            
            # ë°ì´í„°ì…‹ ìƒì„±
            train_dataset = processor.create_dataset(
                texts=train_data['paragraph_text'].tolist(),
                labels=train_data['generated'].tolist()
            )
            
            val_dataset = processor.create_dataset(
                texts=val_data['paragraph_text'].tolist(),
                labels=val_data['generated'].tolist()
            )
            
            # í´ë“œ í›ˆë ¨
            fold_result = self.train_fold(train_dataset, val_dataset, fold_idx)
            self.fold_results.append(fold_result)
        
        # ì „ì²´ ê²°ê³¼ ì •ë¦¬
        cv_results = self._summarize_cv_results()
        
        # ê²°ê³¼ ì €ì¥
        self._save_cv_results(cv_results)
        
        return cv_results
    
    def _summarize_cv_results(self) -> Dict[str, Any]:
        """êµì°¨ê²€ì¦ ê²°ê³¼ ìš”ì•½"""
        fold_aucs = [result['best_metrics']['val_auc'] for result in self.fold_results]
        
        summary = {
            'cv_auc_mean': np.mean(fold_aucs),
            'cv_auc_std': np.std(fold_aucs),
            'cv_auc_scores': fold_aucs,
            'total_training_time': sum(result['training_time'] for result in self.fold_results),
            'config': self.config.to_dict(),
            'fold_results': self.fold_results
        }
        
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ êµì°¨ê²€ì¦ ì™„ë£Œ!")
        self.logger.info("="*60)
        self.logger.info(f"ğŸ“Š CV AUC: {summary['cv_auc_mean']:.4f} Â± {summary['cv_auc_std']:.4f}")
        self.logger.info(f"ğŸ“ˆ í´ë“œë³„ AUC: {[f'{auc:.4f}' for auc in fold_aucs]}")
        self.logger.info(f"â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {summary['total_training_time']:.1f}ì´ˆ")
        
        return summary
    
    def _save_cv_results(self, cv_results: Dict[str, Any]):
        """êµì°¨ê²€ì¦ ê²°ê³¼ ì €ì¥"""
        results_path = os.path.join(self.config.output.log_dir, "cv_results.json")
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(cv_results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {results_path}")


def test_trainer():
    """í›ˆë ¨ê¸° í…ŒìŠ¤íŠ¸"""
    from .config import Config
    
    print("ğŸ§ª KLUE í›ˆë ¨ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì„¤ì • ë¡œë“œ
    config = Config()
    config.training.epochs = 1  # í…ŒìŠ¤íŠ¸ìš©
    config.training.batch_size = 4
    
    # í›ˆë ¨ê¸° ìƒì„±
    trainer = KLUETrainer(config)
    
    print(f"ğŸ¤– ëª¨ë¸: {trainer.model.__class__.__name__}")
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {trainer.device}")
    print(f"ğŸ’¾ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in trainer.model.parameters()):,}")
    
    print("âœ… í›ˆë ¨ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    test_trainer()