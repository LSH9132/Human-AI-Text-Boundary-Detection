"""
KLUE-BERT ì „ìš© ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë©”ì¸ í”„ë¡œì íŠ¸ì˜ ì„±ê³µì ì¸ ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ì„ 
KLUE-BERTì— íŠ¹í™”í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ë¬¸ì„œ ë‹¨ìœ„ ë‹¨ë½ ë¶„í• 
- í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
- Document-Aware ë°ì´í„° ë¶„í• 
- KLUE-BERT í† í¬ë‚˜ì´ì € ìµœì í™”
"""

import pandas as pd
import numpy as np
import re
from typing import List, Dict, Tuple, Optional, Any
import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
import logging
from sklearn.model_selection import StratifiedKFold, GroupKFold

from .config import Config


class KLUETextDataset(Dataset):
    """KLUE-BERT ì „ìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, texts: List[str], labels: Optional[List[int]] = None, 
                 tokenizer: AutoTokenizer = None, max_length: int = 512):
        """
        Args:
            texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            labels: ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸ (0: Human, 1: AI)
            tokenizer: KLUE-BERT í† í¬ë‚˜ì´ì €
            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # í†µê³„ ì •ë³´
        self.total_samples = len(texts)
        if labels is not None:
            self.positive_samples = sum(labels)
            self.negative_samples = self.total_samples - self.positive_samples
    
    def __len__(self) -> int:
        return len(self.texts)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """ë°°ì¹˜ ì•„ì´í…œ ë°˜í™˜"""
        text = str(self.texts[idx])
        
        # KLUE-BERT í† í¬ë‚˜ì´ì§•
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt',
            add_special_tokens=True
        )
        
        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'token_type_ids': encoding.get('token_type_ids', torch.zeros_like(encoding['input_ids'])).flatten()
        }
        
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
        
        return item
    
    def get_stats(self) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ í†µê³„ ë°˜í™˜"""
        stats = {
            'total_samples': self.total_samples,
            'avg_text_length': np.mean([len(text) for text in self.texts])
        }
        
        if self.labels is not None:
            stats.update({
                'positive_samples': self.positive_samples,
                'negative_samples': self.negative_samples,
                'positive_ratio': self.positive_samples / self.total_samples,
                'class_balance_ratio': f"{self.negative_samples}:{self.positive_samples}"
            })
        
        return stats


class KLUEDataProcessor:
    """KLUE-BERT ì „ìš© ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config):
        """
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.model.name,
            use_fast=True
        )
        
        # í†µê³„ ì •ë³´ ì €ì¥
        self.stats = {}
    
    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        self.logger.info("ğŸ“Š ë°ì´í„° íŒŒì¼ ë¡œë”© ì¤‘...")
        
        try:
            # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
            train_df = pd.read_csv(self.config.data.train_file)
            test_df = pd.read_csv(self.config.data.test_file)
            
            self.logger.info(f"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_df):,} ìƒ˜í”Œ")
            self.logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,} ìƒ˜í”Œ")
            
            # ë°ì´í„° ê²€ì¦
            self._validate_data(train_df, test_df)
            
            return train_df, test_df
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def _validate_data(self, train_df: pd.DataFrame, test_df: pd.DataFrame):
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_train_cols = ['title', 'full_text', 'generated']
        required_test_cols = ['title', 'full_text']
        
        missing_train = [col for col in required_train_cols if col not in train_df.columns]
        missing_test = [col for col in required_test_cols if col not in test_df.columns]
        
        if missing_train:
            raise ValueError(f"í›ˆë ¨ ë°ì´í„°ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_train}")
        if missing_test:
            raise ValueError(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_test}")
        
        # ë ˆì´ë¸” ë¶„í¬ í™•ì¸
        label_dist = train_df['generated'].value_counts()
        self.logger.info(f"ğŸ“ˆ ë ˆì´ë¸” ë¶„í¬: {dict(label_dist)}")
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸
        positive_ratio = train_df['generated'].mean()
        self.logger.info(f"ğŸ“Š AI í´ë˜ìŠ¤ ë¹„ìœ¨: {positive_ratio:.1%}")
        
        if positive_ratio < 0.05 or positive_ratio > 0.95:
            self.logger.warning("âš ï¸ ì‹¬ê°í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    def clean_korean_text(self, text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ì œ"""
        if pd.isna(text) or text is None:
            return ""
        
        text = str(text).strip()
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ì œ (í•œêµ­ì–´ ì¹œí™”ì )
        text = re.sub(r'[^\w\sê°€-í£.,!?;:\'"()[\]{}/-]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # ì—°ì†ëœ êµ¬ë‘ì  ì •ë¦¬
        text = re.sub(r'[.]{3,}', '...', text)
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        
        return text.strip()
    
    def split_into_paragraphs(self, text: str, title: str = "") -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë‹¨ë½ìœ¼ë¡œ ë¶„í•  (í•œêµ­ì–´ ìµœì í™”)"""
        if pd.isna(text) or not text:
            return []
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        text = self.clean_korean_text(text)
        
        # ë‹¨ë½ ë¶„í•  (í•œêµ­ì–´ ë¬¸ì¥ êµ¬ë¶„ì ê³ ë ¤)
        paragraphs = []
        
        # ì¤„ë°”ê¿ˆ ê¸°ì¤€ ë¶„í• 
        lines = text.split('\n')
        
        current_paragraph = ""
        for line in lines:
            line = line.strip()
            if not line:
                if current_paragraph:
                    paragraphs.append(current_paragraph)
                    current_paragraph = ""
                continue
            
            # ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ í•©ì¹˜ê¸°
            if len(current_paragraph) < 50:
                current_paragraph += " " + line if current_paragraph else line
            else:
                paragraphs.append(current_paragraph)
                current_paragraph = line
        
        # ë§ˆì§€ë§‰ ë‹¨ë½ ì¶”ê°€
        if current_paragraph:
            paragraphs.append(current_paragraph)
        
        # ë‹¨ë½ í•„í„°ë§
        filtered_paragraphs = []
        for p in paragraphs:
            # ìµœì†Œ ê¸¸ì´ ì²´í¬
            if len(p.strip()) < 20:
                continue
            
            # ìµœëŒ€ í† í° ê¸¸ì´ ì²´í¬ (ëŒ€ëµì )
            if len(p) > self.config.model.max_length * 4:  # í•œêµ­ì–´ëŠ” í‰ê·  4ê¸€ì/í† í°
                # ê¸´ ë‹¨ë½ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¬ë¶„í• 
                sentences = re.split(r'[.!?]\s+', p)
                current_chunk = ""
                for sent in sentences:
                    if len(current_chunk + sent) < self.config.model.max_length * 3:
                        current_chunk += sent + ". "
                    else:
                        if current_chunk.strip():
                            filtered_paragraphs.append(current_chunk.strip())
                        current_chunk = sent + ". "
                if current_chunk.strip():
                    filtered_paragraphs.append(current_chunk.strip())
            else:
                filtered_paragraphs.append(p.strip())
        
        # ìµœëŒ€ ë‹¨ë½ ìˆ˜ ì œí•œ
        if len(filtered_paragraphs) > self.config.data.max_paragraphs_per_doc:
            # ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ê¸´ ë‹¨ë½ë“¤ ì„ íƒ
            filtered_paragraphs.sort(key=len, reverse=True)
            filtered_paragraphs = filtered_paragraphs[:self.config.data.max_paragraphs_per_doc]
        
        return filtered_paragraphs
    
    def preprocess_training_data(self, train_df: pd.DataFrame) -> pd.DataFrame:
        """í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬ - ë‹¨ë½ ë¶„í• """
        self.logger.info("ğŸ”„ í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        processed_data = []
        total_docs = len(train_df)
        
        for idx, row in train_df.iterrows():
            if idx % 10000 == 0:
                self.logger.info(f"ì²˜ë¦¬ ì§„í–‰ë¥ : {idx:,}/{total_docs:,}")
            
            title = str(row['title'])
            full_text = str(row['full_text'])
            label = int(row['generated'])
            
            # ë‹¨ë½ ë¶„í• 
            paragraphs = self.split_into_paragraphs(full_text, title)
            
            # ê° ë‹¨ë½ì„ ê°œë³„ ìƒ˜í”Œë¡œ ì¶”ê°€
            for para_idx, paragraph in enumerate(paragraphs):
                processed_data.append({
                    'title': title,
                    'paragraph_text': paragraph,
                    'paragraph_idx': para_idx,
                    'original_idx': idx,
                    'generated': label
                })
        
        processed_df = pd.DataFrame(processed_data)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['original_documents'] = total_docs
        self.stats['processed_paragraphs'] = len(processed_df)
        self.stats['avg_paragraphs_per_doc'] = len(processed_df) / total_docs
        
        self.logger.info(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {total_docs:,} ë¬¸ì„œ â†’ {len(processed_df):,} ë‹¨ë½")
        self.logger.info(f"ğŸ“Š ë¬¸ì„œë‹¹ í‰ê·  ë‹¨ë½ ìˆ˜: {self.stats['avg_paragraphs_per_doc']:.1f}")
        
        return processed_df
    
    def create_document_aware_splits(self, processed_df: pd.DataFrame) -> List[Tuple[np.ndarray, np.ndarray]]:
        """Document-Aware êµì°¨ê²€ì¦ ë¶„í•  ìƒì„±"""
        self.logger.info("ğŸ“Š Document-Aware êµì°¨ê²€ì¦ ë¶„í•  ìƒì„± ì¤‘...")
        
        # ë¬¸ì„œë³„ ëŒ€í‘œ ë ˆì´ë¸” ê³„ì‚° (ë‹¤ìˆ˜ê²°)
        doc_labels = processed_df.groupby('title')['generated'].agg(lambda x: x.mode().iloc[0])
        
        # GroupKFoldë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‹¨ìœ„ ë¶„í• 
        group_kfold = GroupKFold(n_splits=self.config.cv.n_folds)
        
        splits = []
        for fold_idx, (train_docs, val_docs) in enumerate(group_kfold.split(
            X=doc_labels.index,
            y=doc_labels.values,
            groups=doc_labels.index
        )):
            # ë¬¸ì„œ ì¸ë±ìŠ¤ë¥¼ ë‹¨ë½ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
            train_titles = doc_labels.index[train_docs]
            val_titles = doc_labels.index[val_docs]
            
            train_indices = processed_df[processed_df['title'].isin(train_titles)].index.values
            val_indices = processed_df[processed_df['title'].isin(val_titles)].index.values
            
            splits.append((train_indices, val_indices))
            
            # ë¶„í•  í†µê³„
            train_pos = processed_df.iloc[train_indices]['generated'].sum()
            val_pos = processed_df.iloc[val_indices]['generated'].sum()
            train_total = len(train_indices)
            val_total = len(val_indices)
            
            self.logger.info(
                f"Fold {fold_idx + 1}: "
                f"í›ˆë ¨ {train_total:,}ê°œ (AI: {train_pos:,}, {train_pos/train_total:.1%}) | "
                f"ê²€ì¦ {val_total:,}ê°œ (AI: {val_pos:,}, {val_pos/val_total:.1%})"
            )
        
        return splits
    
    def create_dataset(self, texts: List[str], labels: Optional[List[int]] = None) -> KLUETextDataset:
        """KLUE ë°ì´í„°ì…‹ ìƒì„±"""
        return KLUETextDataset(
            texts=texts,
            labels=labels,
            tokenizer=self.tokenizer,
            max_length=self.config.model.max_length
        )
    
    def get_class_weights(self, labels: List[int]) -> torch.Tensor:
        """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        label_counts = np.bincount(labels)
        total = len(labels)
        
        # ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜
        weights = total / (len(label_counts) * label_counts)
        
        return torch.FloatTensor(weights)
    
    def analyze_text_stats(self, texts: List[str]) -> Dict[str, float]:
        """í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„"""
        if not texts:
            return {}
        
        lengths = [len(text) for text in texts]
        token_counts = [len(self.tokenizer.encode(text, add_special_tokens=False)) for text in texts[:1000]]  # ìƒ˜í”Œë§
        
        return {
            'avg_char_length': np.mean(lengths),
            'median_char_length': np.median(lengths),
            'max_char_length': np.max(lengths),
            'avg_token_count': np.mean(token_counts),
            'max_token_count': np.max(token_counts),
            'texts_over_max_length': sum(1 for tc in token_counts if tc > self.config.model.max_length)
        }
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ë°ì´í„° ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return self.stats.copy()


def test_data_processor():
    """ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    from .config import Config
    
    print("ğŸ§ª KLUE ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ì„¤ì • ë¡œë“œ
    config = Config()
    
    # ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„±
    processor = KLUEDataProcessor(config)
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_text = """
    ì•ˆë…•í•˜ì„¸ìš”. ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
    KLUE-BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ AI ìƒì„± í…ìŠ¤íŠ¸ë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    
    ì´ ì‹œìŠ¤í…œì€ ë¬¸ì„œë¥¼ ë‹¨ë½ìœ¼ë¡œ ë¶„í• í•˜ê³ , ê° ë‹¨ë½ì„ ë…ë¦½ì ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    
    # ë‹¨ë½ ë¶„í•  í…ŒìŠ¤íŠ¸
    paragraphs = processor.split_into_paragraphs(test_text)
    print(f"ğŸ“„ ë¶„í• ëœ ë‹¨ë½ ìˆ˜: {len(paragraphs)}")
    for i, para in enumerate(paragraphs):
        print(f"  {i+1}: {para[:50]}...")
    
    # ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸
    dataset = processor.create_dataset(paragraphs, [0, 1, 0])
    print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    
    # ìƒ˜í”Œ ì•„ì´í…œ í™•ì¸
    sample = dataset[0]
    print(f"ğŸ” ìƒ˜í”Œ í˜•íƒœ: {list(sample.keys())}")
    print(f"   input_ids: {sample['input_ids'].shape}")
    print(f"   attention_mask: {sample['attention_mask'].shape}")
    
    print("âœ… ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    test_data_processor()