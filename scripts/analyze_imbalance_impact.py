#!/usr/bin/env python3
"""
í´ë˜ìŠ¤ ë¶ˆê· í˜• ì˜í–¥ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
í›ˆë ¨ ì™„ë£Œ í›„ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì˜ ë¶ˆê· í˜• ì˜í–¥ì„ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import json
import sys
from pathlib import Path
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns

# Add src to path
sys.path.append(str(Path(__file__).parent.parent / "src"))

def analyze_prediction_distribution(submission_file: str):
    """ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„"""
    print("ğŸ“Š ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„")
    print("=" * 50)
    
    try:
        df = pd.read_csv(submission_file)
        predictions = df['generated'].values
        
        print(f"ì´ ì˜ˆì¸¡ ìƒ˜í”Œ: {len(predictions):,}")
        print(f"ì˜ˆì¸¡ ë²”ìœ„: {predictions.min():.4f} ~ {predictions.max():.4f}")
        print(f"ì˜ˆì¸¡ í‰ê· : {predictions.mean():.4f}")
        print(f"ì˜ˆì¸¡ í‘œì¤€í¸ì°¨: {predictions.std():.4f}")
        
        # ì„ê³„ê°’ë³„ ë¶„ë¥˜ ê²°ê³¼
        thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]
        print(f"\nğŸ“ˆ ì„ê³„ê°’ë³„ AI ë¶„ë¥˜ ë¹„ìœ¨:")
        
        for threshold in thresholds:
            ai_ratio = (predictions > threshold).mean()
            print(f"  ì„ê³„ê°’ {threshold}: {ai_ratio:.2%} AI ë¶„ë¥˜")
        
        # ì˜ˆì¸¡ê°’ êµ¬ê°„ë³„ ë¶„í¬
        bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        hist, _ = np.histogram(predictions, bins=bins)
        
        print(f"\nğŸ“Š ì˜ˆì¸¡ê°’ êµ¬ê°„ë³„ ë¶„í¬:")
        for i, (start, end, count) in enumerate(zip(bins[:-1], bins[1:], hist)):
            percentage = count / len(predictions) * 100
            print(f"  {start:.1f}-{end:.1f}: {count:,}ê°œ ({percentage:.1f}%)")
            
        return {
            'total_samples': len(predictions),
            'mean_prediction': float(predictions.mean()),
            'std_prediction': float(predictions.std()),
            'min_prediction': float(predictions.min()),
            'max_prediction': float(predictions.max()),
            'threshold_analysis': {str(t): float((predictions > t).mean()) for t in thresholds},
            'distribution_bins': {f"{bins[i]:.1f}-{bins[i+1]:.1f}": int(count) for i, count in enumerate(hist)}
        }
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def analyze_class_imbalance_impact():
    """í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ëª¨ë¸ì— ë¯¸ì¹œ ì˜í–¥ ë¶„ì„"""
    print("\nâš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì˜í–¥ ë¶„ì„")
    print("=" * 50)
    
    # ì›ë³¸ í›ˆë ¨ ë°ì´í„° ë¶ˆê· í˜• í™•ì¸
    try:
        train_df = pd.read_csv("data/train.csv")
        class_counts = train_df['generated'].value_counts()
        total = len(train_df)
        
        human_ratio = class_counts[0] / total
        ai_ratio = class_counts[1] / total
        imbalance_ratio = class_counts[0] / class_counts[1]
        
        print(f"ğŸ“‹ í›ˆë ¨ ë°ì´í„° ë¶ˆê· í˜•:")
        print(f"  Human: {class_counts[0]:,}ê°œ ({human_ratio:.2%})")
        print(f"  AI: {class_counts[1]:,}ê°œ ({ai_ratio:.2%})")
        print(f"  ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.1f}:1")
        
        # ë¶ˆê· í˜•ì˜ ì‹¬ê°ë„ í‰ê°€
        if imbalance_ratio > 10:
            severity = "ì‹¬ê°"
            expected_impact = "ë†’ìŒ"
        elif imbalance_ratio > 5:
            severity = "ì¤‘ê°„"
            expected_impact = "ë³´í†µ"
        else:
            severity = "ê²½ë¯¸"
            expected_impact = "ë‚®ìŒ"
        
        print(f"\nğŸ“Š ë¶ˆê· í˜• ì‹¬ê°ë„: {severity}")
        print(f"ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ì˜í–¥: {expected_impact}")
        
        return {
            'human_count': int(class_counts[0]),
            'ai_count': int(class_counts[1]),
            'human_ratio': float(human_ratio),
            'ai_ratio': float(ai_ratio),
            'imbalance_ratio': float(imbalance_ratio),
            'severity': severity,
            'expected_impact': expected_impact
        }
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def analyze_model_bias(submission_file: str):
    """ëª¨ë¸ í¸í–¥ì„± ë¶„ì„"""
    print("\nğŸ¯ ëª¨ë¸ í¸í–¥ì„± ë¶„ì„")
    print("=" * 50)
    
    try:
        df = pd.read_csv(submission_file)
        predictions = df['generated'].values
        
        # 1. ì „ë°˜ì  í¸í–¥ ë¶„ì„
        mean_pred = predictions.mean()
        if mean_pred < 0.3:
            bias_direction = "Human í¸í–¥ (ê°•í•¨)"
            bias_level = "ì‹¬ê°"
        elif mean_pred < 0.4:
            bias_direction = "Human í¸í–¥ (ë³´í†µ)"
            bias_level = "ë³´í†µ"
        elif mean_pred > 0.7:
            bias_direction = "AI í¸í–¥ (ê°•í•¨)"
            bias_level = "ì‹¬ê°"
        elif mean_pred > 0.6:
            bias_direction = "AI í¸í–¥ (ë³´í†µ)"
            bias_level = "ë³´í†µ"
        else:
            bias_direction = "ê· í˜•ì "
            bias_level = "ì–‘í˜¸"
        
        print(f"ğŸ“Š ì „ë°˜ì  í¸í–¥: {bias_direction}")
        print(f"ğŸ“ˆ í¸í–¥ ìˆ˜ì¤€: {bias_level}")
        
        # 2. ê·¹ë‹¨ê°’ ë¶„ì„
        extreme_human = (predictions < 0.1).sum()
        extreme_ai = (predictions > 0.9).sum()
        moderate_range = ((predictions >= 0.3) & (predictions <= 0.7)).sum()
        
        print(f"\nğŸ“Š ì˜ˆì¸¡ ê·¹ë‹¨ì„± ë¶„ì„:")
        print(f"  ë§¤ìš° í™•ì‹¤í•œ Human (<0.1): {extreme_human:,}ê°œ ({extreme_human/len(predictions):.1%})")
        print(f"  ë§¤ìš° í™•ì‹¤í•œ AI (>0.9): {extreme_ai:,}ê°œ ({extreme_ai/len(predictions):.1%})")
        print(f"  ì• ë§¤í•œ êµ¬ê°„ (0.3-0.7): {moderate_range:,}ê°œ ({moderate_range/len(predictions):.1%})")
        
        # 3. ì‹ ë¢°ë„ ë¶„ì„
        confidence_scores = np.abs(predictions - 0.5) * 2  # 0.5ì—ì„œ ê±°ë¦¬ * 2
        avg_confidence = confidence_scores.mean()
        
        print(f"\nğŸ¯ ëª¨ë¸ ì‹ ë¢°ë„:")
        print(f"  í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
        
        if avg_confidence > 0.8:
            confidence_level = "ë§¤ìš° ë†’ìŒ (ê³¼ì‹  ìœ„í—˜)"
        elif avg_confidence > 0.6:
            confidence_level = "ë†’ìŒ"
        elif avg_confidence > 0.4:
            confidence_level = "ë³´í†µ"
        else:
            confidence_level = "ë‚®ìŒ (ë¶ˆí™•ì‹¤)"
        
        print(f"  ì‹ ë¢°ë„ ìˆ˜ì¤€: {confidence_level}")
        
        return {
            'mean_prediction': float(mean_pred),
            'bias_direction': bias_direction,
            'bias_level': bias_level,
            'extreme_human_count': int(extreme_human),
            'extreme_ai_count': int(extreme_ai),
            'moderate_range_count': int(moderate_range),
            'average_confidence': float(avg_confidence),
            'confidence_level': confidence_level
        }
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def suggest_improvements(analysis_results: dict):
    """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê°œì„  ë°©ì•ˆ ì œì•ˆ"""
    print("\nğŸ’¡ ê°œì„  ë°©ì•ˆ ì œì•ˆ")
    print("=" * 50)
    
    suggestions = []
    
    # ë¶ˆê· í˜• ì‹¬ê°ë„ ê¸°ë°˜ ì œì•ˆ
    if analysis_results.get('imbalance_ratio', 0) > 10:
        suggestions.extend([
            "ğŸ¯ Class Weight ì¡°ì • (ê°€ì¤‘ì¹˜ 11.2:1)",
            "ğŸ”¥ Focal Loss ì ìš© (alpha=0.25, gamma=2.0)",
            "ğŸ“Š SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ê³ ë ¤"
        ])
    
    # í¸í–¥ì„± ê¸°ë°˜ ì œì•ˆ
    mean_pred = analysis_results.get('mean_prediction', 0.5)
    if mean_pred < 0.3:
        suggestions.extend([
            "âš–ï¸ ì„ê³„ê°’ ë‚®ì¶”ê¸° (0.5 â†’ 0.3-0.4)",
            "ğŸ¯ AI í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ëŒ€í­ ì¦ê°€",
            "ğŸ“ˆ Recall ì¤‘ì‹¬ í‰ê°€ ì§€í‘œ ì¶”ê°€"
        ])
    elif mean_pred < 0.4:
        suggestions.extend([
            "âš–ï¸ ì„ê³„ê°’ ì¡°ì • (0.5 â†’ 0.4)",
            "ğŸ¯ Class Weight ë¯¸ì„¸ ì¡°ì •"
        ])
    
    # ì‹ ë¢°ë„ ê¸°ë°˜ ì œì•ˆ
    confidence = analysis_results.get('average_confidence', 0.5)
    if confidence < 0.4:
        suggestions.extend([
            "ğŸ”§ ëª¨ë¸ ë³µì¡ë„ ì¦ê°€ (ë” í° BERT ëª¨ë¸)",
            "ğŸ“š ë°ì´í„° ì¦ê°•ìœ¼ë¡œ í•™ìŠµ ì•ˆì •í™”",
            "ğŸ¯ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¬ì¡°ì •"
        ])
    elif confidence > 0.8:
        suggestions.extend([
            "âš ï¸ ê³¼ì í•© í™•ì¸ í•„ìš”",
            "ğŸ¯ ì •ê·œí™” ê°•í™” (dropout, weight decay)",
            "ğŸ“Š CVì™€ Public ì ìˆ˜ ì°¨ì´ ëª¨ë‹ˆí„°ë§"
        ])
    
    # ê·¹ë‹¨ê°’ ë¶„ì„ ê¸°ë°˜ ì œì•ˆ
    extreme_ratio = analysis_results.get('extreme_human_count', 0) + analysis_results.get('extreme_ai_count', 0)
    total_samples = analysis_results.get('total_samples', 1)
    
    if extreme_ratio / total_samples > 0.8:
        suggestions.append("âš ï¸ ê³¼ë„í•œ í™•ì‹  - Temperature Scaling ê³ ë ¤")
    elif extreme_ratio / total_samples < 0.2:
        suggestions.append("ğŸ¯ ëª¨ë¸ ê²°ì •ë ¥ í–¥ìƒ - ë” ê¹Šì€ í•™ìŠµ í•„ìš”")
    
    print("ğŸ“‹ ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ë°©ì•ˆ:")
    for i, suggestion in enumerate(suggestions[:8], 1):  # ìƒìœ„ 8ê°œë§Œ
        print(f"  {i}. {suggestion}")
    
    if not suggestions:
        print("  âœ… í˜„ì¬ ëª¨ë¸ì´ ê· í˜•ì ìœ¼ë¡œ ì˜ ì‘ë™ ì¤‘")
    
    return suggestions

def generate_improvement_commands(analysis_results: dict):
    """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ êµ¬ì²´ì ì¸ ì‹¤í–‰ ëª…ë ¹ì–´ ìƒì„±"""
    print("\nğŸš€ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ì–´")
    print("=" * 50)
    
    commands = []
    
    # ë¶ˆê· í˜• ë¹„ìœ¨ì— ë”°ë¥¸ ëª…ë ¹ì–´
    imbalance_ratio = analysis_results.get('imbalance_ratio', 1)
    
    if imbalance_ratio > 10:
        commands.extend([
            "# Class Weight ì¡°ì • í›ˆë ¨",
            "python main.py --env gpu --experiment class_weight",
            "",
            "# Focal Loss ì ìš© í›ˆë ¨", 
            "python main.py --env gpu --experiment focal_loss",
            "",
            "# ì„ê³„ê°’ ìµœì í™”",
            "python scripts/optimize_threshold.py --input submission.csv --target recall"
        ])
    
    # í¸í–¥ì„±ì— ë”°ë¥¸ ëª…ë ¹ì–´
    mean_pred = analysis_results.get('mean_prediction', 0.5)
    
    if mean_pred < 0.4:
        optimal_threshold = max(0.2, mean_pred - 0.1)
        commands.extend([
            "",
            f"# ì„ê³„ê°’ ì¡°ì • ì˜ˆì¸¡",
            f"python scripts/adjust_threshold.py --input submission.csv --threshold {optimal_threshold:.2f}"
        ])
    
    # SMOTE ëª…ë ¹ì–´ (ì‹¬ê°í•œ ë¶ˆê· í˜• ì‹œ)
    if imbalance_ratio > 8:
        commands.extend([
            "",
            "# SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ì ìš©",
            "python scripts/train_with_smote.py --env gpu --sampling_strategy auto"
        ])
    
    print("```bash")
    for command in commands:
        print(command)
    print("```")
    
    return commands

def save_analysis_report(all_results: dict, output_file: str = "results/imbalance_analysis_report.json"):
    """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    Path("results").mkdir(exist_ok=True)
    
    report = {
        'analysis_timestamp': pd.Timestamp.now().isoformat(),
        'baseline_performance': all_results,
        'improvement_priority': [
            "class_weight_adjustment",
            "focal_loss_implementation", 
            "threshold_optimization",
            "smote_oversampling",
            "advanced_augmentation"
        ],
        'next_experiments': [
            "python main.py --env gpu --experiment class_weight",
            "python main.py --env gpu --experiment focal_loss", 
            "python scripts/optimize_threshold.py --input submission.csv"
        ]
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {output_file}")

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
    print("ğŸ” í´ë˜ìŠ¤ ë¶ˆê· í˜• ì˜í–¥ ì¢…í•© ë¶„ì„")
    print("=" * 60)
    
    # ì œì¶œ íŒŒì¼ í™•ì¸
    submission_files = ["submission.csv", "submissions/submission_*.csv"]
    submission_file = None
    
    for pattern in submission_files:
        if "*" in pattern:
            # Glob íŒ¨í„´ìœ¼ë¡œ ìµœì‹  íŒŒì¼ ì°¾ê¸°
            from glob import glob
            files = glob(pattern)
            if files:
                submission_file = max(files, key=lambda x: Path(x).stat().st_mtime)
                break
        else:
            if Path(pattern).exists():
                submission_file = pattern
                break
    
    if not submission_file:
        print("âŒ ì œì¶œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ì˜ˆìƒ ìœ„ì¹˜: submission.csv ë˜ëŠ” submissions/submission_*.csv")
        return 1
    
    print(f"ğŸ“„ ë¶„ì„ ëŒ€ìƒ: {submission_file}")
    
    # ì¢…í•© ë¶„ì„ ì‹¤í–‰
    all_results = {}
    
    # 1. ì˜ˆì¸¡ê°’ ë¶„í¬ ë¶„ì„
    pred_analysis = analyze_prediction_distribution(submission_file)
    if pred_analysis:
        all_results['prediction_distribution'] = pred_analysis
    
    # 2. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì˜í–¥ ë¶„ì„  
    imbalance_analysis = analyze_class_imbalance_impact()
    if imbalance_analysis:
        all_results['class_imbalance'] = imbalance_analysis
    
    # 3. ëª¨ë¸ í¸í–¥ì„± ë¶„ì„
    bias_analysis = analyze_model_bias(submission_file)
    if bias_analysis:
        all_results['model_bias'] = bias_analysis
    
    # ê²°í•©ëœ ê²°ê³¼ë¡œ ê°œì„  ë°©ì•ˆ ì œì•ˆ
    combined_results = {}
    for analysis in all_results.values():
        combined_results.update(analysis)
    
    # 4. ê°œì„  ë°©ì•ˆ ì œì•ˆ
    suggestions = suggest_improvements(combined_results)
    all_results['improvement_suggestions'] = suggestions
    
    # 5. ì‹¤í–‰ ëª…ë ¹ì–´ ìƒì„±
    commands = generate_improvement_commands(combined_results)
    all_results['execution_commands'] = commands
    
    # 6. ë³´ê³ ì„œ ì €ì¥
    save_analysis_report(all_results)
    
    print("\n" + "=" * 60)
    print("âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• ì˜í–¥ ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„: IMBALANCE_IMPROVEMENT_PLAN.md ì°¸ì¡°")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())